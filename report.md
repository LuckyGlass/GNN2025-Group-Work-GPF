# Graph Prompt Tuning 实验报告：从 GPF 到 EdgePrompt

## 1 介绍

Prompt tuning 原是应用于大语言模型的方法。现有大模型具有强大的指令遵循能力，能够根据输入的文字理解任务，因此用户可以用输入文字定制大模型的功能，该段引导大模型理解问题、解决任务的提示性文本称为 Prompt。随着大模型发展，一类方法探索非自然语言、非人工设计的“Prompt”，统称为 Soft prompt。具体而言，大模型将自然语言转换为 Embedding 后进行相关运算，因此这一类方法探索直接将一个连续向量序列作为 Prompt embedding 输入模型，进而用多种学习方法自动优化 Soft prompt。

Prompt tuning 由于可修改参数量低，不直接修改预训练模型权重，在一定程度上避免了过拟合风险，并能高效适配下游任务。

在图神经网络（GNN）领域，Prompt tuning 本质上是通过修改输入图数据（特征或结构），而非修改模型权重，来提升下游任务能力。例如，一类启发式的方法是计算每个节点的多跳闭包，加边连接闭包中心和闭包的其他节点。然而启发式的方法不一定适用于任意下游任务。

本报告涵盖了两项关于图提示学习（Graph Prompt Tuning）的复现与探究工作：
1.  **Universal Prompt Tuning (GPF):** 探索了基于节点特征扰动的提示方法，并提出了改进方案 GPF-Multi。
2.  **EdgePrompt:** 复现了 ICLR 2025 的最新工作，该方法从边（Edge）的视角设计提示，本实验深入探究了不同的提示注入策略对模型性能的影响。

## 2 第一部分：Universal Prompt Tuning 复现与改进

### 2.1 Universal Prompt Tuning for Graph Neural Networks

该论文提出了 GPF 方法。虽然对图输入的修改涉及对点特征的修改和对图结构的修改，但对图结构的修改涉及对任意两点间的连边的预测，复杂度不可接受，GPF 仅考虑修改点特征。具体而言，针对给定下游任务，GPF 学习一个和输入特征等维度的特征增量 $p$，设输入的节点特征为 $(x_1,\dots,x_N)$（$N$ 为节点总数）。GPF 更新节点特征为 $(x_1+p,\dots,x_N+p)$，其余不改变，将更新的输入传入预训练的模型（连接下游任务的输出头）得到模型预测。

$p$ 与下游任务输出头同时在下游任务的训练集上优化。

除此之外，该论文提出了方法 GPF-Plus 作为改进。单个特征增量 $p$ 容量非常局限，GPF-Plus 学习 $K$ 个特征增量 $p_1,\dots,p_K$ 以及与之相关的索引 $k_1,\dots,k_K$，更新的点特征为
$$
x'_i=\frac{\sum_{j=1}^{K}\exp\{k_j^Tx_i\}p_j}{\sum_{j=1}^K\exp\{k_j^Tx_i\}}.
$$

该论文证明了在特征容量足够的前提下，任意对图输入的修改（包括对图结构的修改）等价于一种仅对节点特征的修改，从而保证了“仅修改点特征”这一简化不会损伤 Prompt tuning 的表达能力。本质上，修改模型结构是修改了图网络中消息传递的路径，影响节点的隐层特征，而修改输入特征可以达到相同的效果。

### 2.2 方法改进

GPF 仅在输入层添加特征增量，虽然理论上具有足够的表达能力，但在实践中发现单一的输入扰动需要通过多层传播才能影响最终输出，优化难度较大，且不同层的特征具有不同的语义层次，统一的输入扰动难以精细调控各层表示。直观的，增加可微调的参数量能够进一步提升 Prompt tuning 的表达能力。GPF-Plus 提出优化多个特征增量，然而它仍然局限于对输入的修改。由于模型的权重不变、图结构不变，修改输入对输出的影响局限且不直观，我们提出在图网络的输入层以及所有隐层上添加特征增量。

设图网络的层数为 $L$，第 $l$ 层的输出记为 $h^{(l)}_{i}$，简便起见，记输入 $h_i^{(0)}:=x_i$；第 $l$ 层接受 $l-1$ 层的所有节点特征以及图结构，输出第 $l$ 层的节点特征，记为 $(h_{1}^{(l)},\dots,h_{N}^{(l)})=F_{l}(h_{1}^{(l-1)},\dots,h_{N}^{(l-1)};G)$。我们修改 $F_l$ 的输入：
$$
(h_1^{(l)},\dots,h_N^{(l)})=F_{l}(h_{1}^{(l)}+p^{(l)},\dots,h_{N}^{(l)}+p^{(l)};G)
$$
其中 $p^{(l)}$ 是与 $h_{i}^{(l)}$ 等维度的隐层特征增量。

### 2.3 实验

我们复现了 GPF 在生物领域的部分下游任务上的结果，并在相同的任务上与我们改进的方法做对比。GPF 在多种预训练模型上进行实验，保证方法的泛用性；我们选取了其中的两种，Infomax 和 AttrMasking，使用相同的预训练模型权重，与 GPF 进行对比。

在每组实验（一种预训练方法和一个下游任务）中，我们采用了 5 个不同的随机数种子进行重复实验，保证实验结论的可靠性。下表汇报了每组实验在 5 个随机数种子下的指标的平均值，不同随机数种子的实验结果详见附录。

| Pretrain    | Algorithm        |     BBBP     |    Tox21     |   ToxCast    |    SIDER     |   ClinTox    |     BACE     |
| :---------- | :--------------- | :----------: | :----------: | :----------: | :----------: | :----------: | :----------: |
| Infomax     | GPF (Original)   | <u>66.83</u> |  **79.09**   |  **66.10**   |  **66.17**   | <u>73.56</u> |  **83.60**   |
|             | GPF (Reproduced) |    65.51     |    77.58     |    65.23     | <u>65.57</u> |    73.32     |    82.16     |
|             | GPF-Multi        |  **67.76**   | <u>78.19</u> | <u>65.97</u> |    64.98     |  **76.58**   | <u>83.35</u> |
| AttrMasking | GPF (Original)   |  **68.09**   | <u>79.04</u> | <u>66.32</u> |  **69.13**   |    75.06     |  **84.33**   |
|             | GPF (Reproduced) | <u>67.29</u> |    78.57     |    65.77     |    68.35     | <u>75.31</u> |    83.82     |
|             | GPF-Multi        |    67.11     |  **79.44**   |  **66.46**   | <u>68.46</u> |  **75.67**   | <u>83.84</u> |

改进的方法在绝大多数实验中表现优于 GPF 的复现结果。我们按照 GPF 作者开源的代码复现了结果，然而多数任务上没有达到论文上汇报的结果。考虑硬件条件、具体实验设置的匹配性，我们认为将改进的方法与复现的结果作对比更合理，因此我们认为本实验提出的改进方法能够带来稳定提升。

上述所有实验在单张 4090 上完成，所有实验总时长约 75 GPU小时。

## 3 第二部分：EdgePrompt 复现与分析

### 3.1 论文方法

EdgePrompt (ICLR 2025) 提出了一种基于边的图提示微调方法。不同于以往在节点特征上添加提示向量的方法，EdgePrompt 旨在通过学习边的提示向量来操纵输入图，并将其融入消息传递机制中。该方法包含两个变体：
*   **EdgePrompt:** 为每一层的边学习一个共享的全局提示向量。
*   **EdgePrompt+:** 引入锚点（Anchor Prompts）机制，通过计算边两端节点与锚点的关系，为每条边生成定制化的提示向量。

### 3.2 EdgePrompt vs. GPF：理论与论文基准对比

在进行复现实验之前，我们首先依据 EdgePrompt 原论文中的理论分析与实验数据，建立 EdgePrompt 与前人工作 GPF 的对比基准。

#### 3.2.1 理论机制差异
*   **GPF (Node-level):** GPF 学习一个统一的提示向量 $p$ 加到所有节点特征上。在 GCN 等模型的消息传递中，这导致了一个问题：对于任意节点 $v_i$，它的所有邻居收到的来自 $v_i$ 的信息都被加上了相同的 $p$。这种“均匀传播”忽略了不同邻居节点可能需要不同上下文信息的特性。
*   **EdgePrompt (Edge-level):** EdgePrompt 将提示向量 $e_{ij}$ 作用于边上。通过 EdgePrompt+ 的锚点机制，每一条边 $(v_i, v_j)$ 都能获得定制化的提示。这意味着节点 $v_i$ 在向 $v_j$ 和 $v_k$ 传递消息时，可以附带不同的提示信息，从而更灵活地捕捉图结构信息。原论文通过定理证明，EdgePrompt 在提升类别间线性可分性方面优于仅修改节点特征的方法。

#### 3.2.2 论文实验数据对比
根据 EdgePrompt 论文（Table 2 & Table 3），在 GraphCL 预训练策略下，EdgePrompt+ 在多个数据集上均优于 GPF 和 GPF-plus。以下是论文汇报的 5-shot 节点分类准确率对比：

| 数据集 | GPF | GPF-plus | EdgePrompt | EdgePrompt+ | 提升 (vs GPF) |
|:---|:---:|:---:|:---:|:---:|:---:|
| **Cora** | 58.52 | 52.24 | 58.60 | **62.88** | +4.36 |
| **CiteSeer** | 43.55 | 38.47 | 43.31 | **46.20** | +2.65 |
| **Pubmed** | 67.67 | 64.30 | 67.76 | 67.41 | -0.26 |
| **ogbn-arxiv**| 21.73 | 21.03 | 21.90 | **23.18** | +1.45 |
| **Flickr** | 23.98 | 25.32 | 24.83 | **25.57** | +1.59 |

### 3.3 实验设置与探究方向

本部分实验旨在验证 EdgePrompt 的有效性，并重点探究不同的提示注入方式（Prompt Insertion Strategy）对模型性能的影响，这是原论文中未详细讨论的关键细节。

*   **数据集：** Cora, CiteSeer, PubMed, ogbn-arxiv (节点分类任务)。
*   **实验设定：** 5-shot 节点分类，预训练模型为 GraphCL，Backbone 为 2 层 GCN（隐藏维度 128）。
*   **EdgePrompt+ 设置：** 使用 10 个锚点。
*   **训练配置：** 200 epochs，学习率 0.001，5 次随机种子取平均。

原论文中只有加法这一种注入方式，我们额外实现了乘法和仿射这两种方式，对比了三种不同的提示注入方式（$x_j$ 为邻居节点特征，`edge_attr` 为学习到的边提示）：
1.  **Add (加法):** $\text{msg}=x_j+\mathrm{edge\_attr}$。类似于残差连接，直接叠加特征。
2.  **Mul (乘法):** $\text{msg}=x_j\times (1 + \mathrm{edge\_attr})$。将提示视为门控信号或缩放因子。
3.  **Affine (仿射):** $\text{msg} = \alpha \cdot x_j + \mathrm{edge\_attr}$。引入可学习权重 $\alpha$。

### 3.4实验结果 

下表展示了不同数据集上 EdgePrompt 和 EdgePrompt+ 在不同注入方式下的 5-shot 分类准确率 (%)。

| 数据集         | 方法        | Add (加法) | Mul (乘法) | Affine (仿射) | 论文汇报结果 |
| :------------- | :---------- | :--------: | :--------: | :-----------: | :----------: |
| **Cora**       | EdgePrompt  |   55.90    |   52.50    |     55.08     |      -       |
|                | EdgePrompt+ | **60.41**  |   54.23    |     60.07     |    62.88     |
| **CiteSeer**   | EdgePrompt  |   40.63    |   38.02    |     39.94     |      -       |
|                | EdgePrompt+ | **43.37**  |   38.35    |     43.58     |    46.20     |
| **PubMed**     | EdgePrompt  |   65.02    |   64.61    |     64.84     |      -       |
|                | EdgePrompt+ | **66.09**  |   64.52    |     66.05     |    67.41     |
| **ogbn-arxiv** | EdgePrompt  |   19.63    |   19.63    |     19.40     |      -       |
|                | EdgePrompt+ | **20.01**  |   19.45    |     19.56     |    23.18     |

### 3.5 结果分析

**1. 注入方式的影响：**
实验表明，**Add (加法) 注入方式在绝大多数情况下表现最优且最稳定**。
*   **Mul 方式的局限性：** 在 Cora 和 CiteSeer 等小规模稀疏图上，Mul 方式显著劣于 Add 方式（差距达 4-6%）。原因可能在于：GCN 本身包含度归一化操作，乘法形式的提示可能破坏了图的结构信息；此外，如果学习到的 `edge_attr` 导致缩放因子接近 0，会造成严重的信息丢失。在 PubMed 这种较密集的图上，其负面影响相对较小。
*   **Add 方式的优势：** 类似于 ResNet 的残差思想，加法注入即使在提示学习不完善时，也不会破坏原始的节点特征，梯度回传也更为顺畅，因此训练收敛性更好。

**2. EdgePrompt vs EdgePrompt+：**
EdgePrompt+ 在所有数据集上均优于基础版 EdgePrompt。
*   **提升幅度：** 在 Cora 上提升最显著 (+4.51%)，说明在样本稀缺的小图上，基于锚点的细粒度提示更有价值。
*   **大规模图的挑战：** 在 ogbn-arxiv 上提升微弱，且整体准确率较低 (~20%)。这表明在超大规模图上进行 5-shot 学习极具挑战性，且 10 个锚点的设置可能不足以覆盖复杂的大图语义。

**3. 与原论文结果对比：**
我们的复现结果（Add 方式）与原论文汇报结果存在约 2.5% 的差距。这属于合理范围，可能源于预训练模型权重的具体差异、数据划分（Split）的随机性以及未公开的超参数细节（如学习率衰减策略）。但各方法的相对排名与论文一致，验证了 EdgePrompt+ 的有效性。

## 4 总结

本报告通过两部分实验系统地研究了图神经网络的提示微调技术。
1.  **对于 GPF：** 我们验证了基于节点特征扰动的有效性，并提出在所有隐层注入提示的 GPF-Multi 方法，实验证明该改进在生物化学分子图任务上能带来比单层提示更优的性能。
2.  **对于 EdgePrompt：** 我们证实了从边视角设计提示的可行性。通过对比实验发现，将提示向量以**加法（Add）**形式注入消息传递机制是最稳健的策略，而乘法策略容易导致训练不稳定和性能下降。EdgePrompt+ 凭借其细粒度的锚点机制，在少样本节点分类任务上表现出优于基础版本的性能。

## 附录

### A. GPF 复现与改进详细实验数据 (不同随机种子)

|Pretrain|  Method	|Seed	|BBBP	|Tox21	|ToxCast	|SIDER	|ClinTox	|BACE|
|:---|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|Infomax	|GPF (Original)	|	|66.83 	|79.09 	|66.10 	|66.17 	|73.56 	|83.60 |
|	|GPF (Reproduced)	|42	|64.37 	|77.06 	|65.32 	|66.75 	|75.08 	|82.02| 
|	|	|77	|67.22 	|78.16 	|65.02 	|65.06 	|68.89 	|81.85| 
|	|	|728	|64.22 	|77.69 	|65.09 	|64.93 	|74.17 	|82.98| 
|	|	|2025	|65.55 	|77.73 	|65.48 	|65.36 	|73.63 	|82.00| 
|	|	|1234	|66.17 	|77.24 	|65.23 	|65.77 	|74.82 	|81.95| 
|	|	|avg	|65.51 	|77.58 	|65.23 	|65.57 	|73.32 	|82.16| 
|	|	|std	|1.26 	|0.43 	|0.18 	|0.73 	|2.54 	|0.46| 
|	|GPF-Multi	|42	|67.64 	|78.11 	|66.20 	|64.61 	|78.39 	|83.60| 
|	|	|77	|67.63 	|77.98 	|65.92 	|65.16 	|79.82 	|83.05| 
|	|	|728	|69.16 	|78.02 	|65.54 	|65.71 	|75.07 	|83.93| 
|	|	|2025	|66.66 	|78.24 	|66.09 	|64.96 	|76.36 	|83.29| 
|	|	|1234	|67.69 	|78.58 	|66.13 	|64.44 	|73.27 	|82.89| 
|	|	|avg	|67.76 	|78.19 	|65.97 	|64.98 	|76.58 	|83.35| 
|	|	|std	|0.90 	|0.24 	|0.26 	|0.50 	|2.60 	|0.42| 
|AttrMasking|	GPF (Original)|	|68.09 	|79.04 	|66.32 	|69.13 	|75.06 	|84.33| 
|	|GPF (Reproduced)	|42	|67.93 	|78.26 	|66.10 	|67.79 	|65.69 	|83.34| 
|	|	|77	|65.58 	|78.43 	|66.45 	|69.26 	|74.60 	|84.44| 
|	|	|728	|68.02 	|78.55 	|65.53 	|69.68 	|79.70 	|83.38| 
|	|	|2025	|66.10 	|78.40 	|65.39 	|67.70 	|77.83 	|83.05| 
|	|	|1234	|68.81 	|79.23 	|65.38 	|67.32 	|78.72 	|84.89| 
|	|	|avg	|67.29 	|78.57 	|65.77 	|68.35 	|75.31 	|83.82| 
|	|	|std	|1.38 	|0.38 	|0.48 	|1.05 	|5.71 	|0.80| 
|	|GPF-Multi	|42	|67.58 	|79.64 	|66.85 	|69.53 	|76.78 	|84.14| 
|	|	|77	|65.44 	|79.34 	|66.36 	|68.92 	|74.86 	|84.61| 
|	|	|728	|66.96 	|79.58 	|66.21 	|67.71 	|76.49 	|83.64| 
|	|	|2025	|68.03 	|79.31 	|66.34 	|67.86 	|74.71 	|82.75| 
|	|	|1234	|67.55 	|79.33 	|66.55 	|68.26 	|75.53 	|84.07| 
|	|	|avg	|67.11 	|79.44 	|66.46 	|68.46 	|75.67 	|83.84| 
|	|	|std	|1.01 	|0.15 	|0.25 	|0.76 	|0.93 	|0.70| 

### B. EdgePrompt 详细实验数据 (Cora 数据集示例)

| 方法        | 注入方式 | 准确率 (%) | 标准差 (%) |
| ----------- | -------- | ---------- | ---------- |
| EdgePrompt  | add      | 55.90      | 4.41       |
| EdgePrompt  | affine   | 55.08      | 4.96       |
| EdgePrompt  | mul      | 52.50      | 5.02       |
| EdgePrompt+ | add      | **60.41**  | 6.16       |
| EdgePrompt+ | affine   | 60.07      | 6.35       |
| EdgePrompt+ | mul      | 54.23      | 5.48       |

